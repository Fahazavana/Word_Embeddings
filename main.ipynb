{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"data/\"\n",
    "TRAIN = f\"{DATA}/train.csv\"\n",
    "TEST = f\"{DATA}/test.csv\"\n",
    "TRAIN_LABEL = f\"{DATA}/train_label.txt\"\n",
    "TRAIN_TITLE = f\"{DATA}/train_title.txt\"\n",
    "TRAIN_TEXT = f\"{DATA}/train_text.txt\"\n",
    "TEST_LABEL = f\"{DATA}/test_label.txt\"\n",
    "TEST_TITLE= f\"{DATA}/test_title.txt\"\n",
    "TEST_TEXT = f\"{DATA}/test_text.txt\"\n",
    "\n",
    "TRAIN_NORM = f\"{DATA}/train_norm.txt\"\n",
    "TEST_NORM = f\"{DATA}/test_norm.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Splt the csv into label, title, text\n",
    "# import pandas as pd\n",
    "\n",
    "# train = pd.read_csv(TRAIN)\n",
    "# train_col = train.columns\n",
    "# train[train_col[0]].to_csv(TRAIN_LABEL, index=False)\n",
    "# train[train_col[1]].to_csv(TRAIN_TITLE, index=False)\n",
    "# train[train_col[2]].to_csv(TRAIN_TEXT, index=False)\n",
    "\n",
    "# test = pd.read_csv(TEST)\n",
    "# test_col = test.columns\n",
    "# test[test_col[0]].to_csv(TEST_LABEL, index=False)\n",
    "# test[test_col[1]].to_csv(TEST_TITLE, index=False)\n",
    "# test[test_col[2]].to_csv(TEST_TEXT, index=False)\n",
    "# del train, test, pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.Normalizer import normalize_data\n",
    "# normalize_data(TRAIN_TEXT, TRAIN_NORM)\n",
    "# normalize_data(TEST_TEXT, TEST_NORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Union, Generator\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "\tdef __init__(self, file_name, window_size=1):\n",
    "\n",
    "\t\tself.file_name = file_name\n",
    "\t\tself.window_size = window_size\n",
    "\t\tself.word2id = {}\n",
    "\t\tself.id2word = {}\n",
    "\t\tself.word_count = {}\n",
    "\t\tself.word_prob = {}\n",
    "\t\tself.total_count= 0\n",
    "\t\tself.vocab_size = 0\n",
    "\t\tself.data: Union[List, np.ndarray] = []\n",
    "\t\tself.__build_data()\n",
    "\n",
    "\tdef __iter__(self):\n",
    "\t\twith open(self.file_name, \"r\") as file:\n",
    "\t\t\tyield from file\n",
    "\n",
    "\tdef __update_map(self, text):\n",
    "\t\tfor word in text:\n",
    "\t\t\tif word not in self.word2id.keys():\n",
    "\t\t\t\tself.word2id[word] = self.vocab_size\n",
    "\t\t\t\tself.id2word[self.vocab_size] = word\n",
    "\t\t\t\tself.word_count[word] = 0\n",
    "\t\t\t\tself.vocab_size += 1\n",
    "\t\t\tself.word_count[word]+=1\n",
    "\n",
    "\tdef __write_pairs(self, text):\n",
    "\t\tnum_words = len(text)\n",
    "\t\tfor i, word in enumerate(text):\n",
    "\t\t\tcenter = self.word2id[word]\n",
    "\t\t\tnum_words = len(text)\n",
    "\t\t\tcontext = (\n",
    "\t\t\t\ttext[max(0, i - self.window_size) : i]\n",
    "\t\t\t\t+ text[i + 1 : min(num_words, i + self.window_size + 1)]\n",
    "\t\t\t)\n",
    "\t\t\tself.data.extend(((center, self.word2id[cnt]) for cnt in context))  # type: ignore\n",
    "\n",
    "\tdef __build_data(self) -> None:\n",
    "\t\tfor line in self:\n",
    "\t\t\ttext = line.strip().split()\n",
    "\t\t\ttext = [word.strip() for word in text]\n",
    "\t\t\tself.__update_map(text)\n",
    "\t\t\tself.__write_pairs(text)\n",
    "\t\tself.data = np.array(list(set(self.data)), dtype=\"int32\")\n",
    "\t\tself.total_count = np.sum(list(self.word_count.values()))\n",
    "\t\t_tmp = np.array(list(self.word_count.values()))**(0.75)\n",
    "\t\t_tmp = np.sum(_tmp)\n",
    "\t\tself.word_prob = {self.word2id[k]:v**0.75/_tmp for k, v in self.word_count.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTINGDATA = f\"{DATA}/testing_data.txt\"\n",
    "testing_corpus = Corpus(TRAIN_NORM, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNS(nn.Module):\n",
    "\tdef __init__(self, num_words, emb_dim):\n",
    "\t\tsuper(SGNS, self).__init__()\n",
    "\t\tself.num_words = num_words\n",
    "\t\tself.emb_dim = emb_dim\n",
    "\t\tself.center = nn.Embedding(num_words, emb_dim)\n",
    "\t\tself.context = nn.Embedding(num_words, emb_dim)\n",
    "\n",
    "\tdef forward(self, center, context):\n",
    "\t\tcenter = self.center(center)\n",
    "\t\tcontext = self.context(context)\n",
    "\t\toutput = torch.matmul(context, center.mT).squeeze(1)\n",
    "\t\treturn output\n",
    "\n",
    "\tdef get_similarity(self, idx):\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tcenter_emb = self.center.weight[idx]\n",
    "\t\t\tsimilarities = torch.cosine_similarity(\n",
    "\t\t\t\tcenter_emb.unsqueeze(0), self.center.weight, dim=1\n",
    "\t\t\t)\n",
    "\t\treturn similarities\n",
    "\n",
    "\tdef get_cosine_distance(self, idx):\n",
    "\t\tsimilarities = self.get_similarity(idx)\n",
    "\t\treturn 1 - similarities\n",
    "\n",
    "\n",
    "class CorpusData(Dataset):\n",
    "\tdef __init__(self, corpus, k):\n",
    "\t\tself.data = torch.from_numpy(corpus.data)\n",
    "\t\tself.V = corpus.vocab_size\n",
    "\t\tself.noise = torch.from_numpy(np.array(list(corpus.word_prob.values())))\n",
    "\t\tself.k = k\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\tc_positive, o_positive = self.data[idx]\n",
    "\t\t# _tmp = self.noise.clone()\n",
    "\t\t# _tmp[o_positive] = 0.0\n",
    "\t\to_negative = torch.multinomial(input = self.noise, num_samples = self.k, replacement = True)\n",
    "\t\tc_negative = c_positive * torch.ones_like(o_negative, dtype=torch.long)\n",
    "\t\treturn (c_positive, o_positive), (c_negative, o_negative)\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.data)\n",
    "\n",
    "class SGNSLoss(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(SGNSLoss, self).__init__()\n",
    "\n",
    "\tdef forward(self, positive, negatives):\n",
    "\t\ta = positive.sigmoid().log().squeeze()\n",
    "\t\tb = (-negatives).sigmoid().log()\n",
    "\t\treturn torch.mean(-a -b)\n",
    "\n",
    "def train(model,criterion, optimizer, dataloader, epochs):\n",
    "\tN= len(dataloader.dataset)\n",
    "\tfor i in range(epochs):\n",
    "\t\tlog = []\n",
    "\t\ttotal_loss = 0\n",
    "\t\tmodel.train()\n",
    "\t\tpbar = tqdm(dataloader, total=len(dataloader), desc=f\"Epoch {i+1}\")\n",
    "\t\tfor p, n in pbar:\n",
    "\t\t\tpos_out = model(p[0].unsqueeze(1), p[1].unsqueeze(1))\n",
    "\t\t\tneg_out = model(n[0].unsqueeze(2), n[1].unsqueeze(2))\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss = criterion(pos_out, neg_out)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\ttotal_loss += loss.item() * p[0].size(0)\n",
    "\n",
    "\t\t\tpbar.set_postfix(loss=f\"{total_loss/N:.2f}\")\n",
    "\n",
    "\t\tavg_loss = total_loss / N\n",
    "\t\tlog.append(avg_loss)\n",
    "\treturn model, log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNSLoss(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(SGNSLoss, self).__init__()\n",
    "\n",
    "\tdef forward(self, positive, negatives):\n",
    "\t\ta = positive.sigmoid().log().squeeze()\n",
    "\t\tb = (-negatives).sigmoid().log()\n",
    "\t\treturn torch.mean(-a -b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_data =  CorpusData(testing_corpus, 3)\n",
    "dataloader = DataLoader(corpus_data, batch_size=4096, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = testing_corpus.vocab_size\n",
    "model = SGNS(N, 25)\n",
    "model = model.to(device)\n",
    "# data = next(iter(corpus_loader))\n",
    "criterion = SGNSLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101:  12%|█▏        | 141/1180 [00:45<05:35,  3.10it/s, loss=40.99]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(dataloader, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, n \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m      7\u001b[0m \tpos_out \u001b[38;5;241m=\u001b[39m model(p[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), p[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m      8\u001b[0m \tneg_out \u001b[38;5;241m=\u001b[39m model(n[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), n[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/torch/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/torch/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/torch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.14/envs/torch/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m, in \u001b[0;36mCorpusData.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     35\u001b[0m c_positive, o_positive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[idx]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# _tmp = self.noise.clone()\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# _tmp[o_positive] = 0.0\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m o_negative \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplacement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m c_negative \u001b[38;5;241m=\u001b[39m c_positive \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(o_negative, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (c_positive, o_positive), (c_negative, o_negative)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(100, 200):\n",
    "\tlog = []\n",
    "\ttotal_loss = 0\n",
    "\tmodel.train()\n",
    "\tpbar = tqdm(dataloader, total=len(dataloader), desc=f\"Epoch {i+1}\")\n",
    "\tfor p, n in pbar:\n",
    "\t\tpos_out = model(p[0].unsqueeze(1).to(device), p[1].unsqueeze(1).to(device))\n",
    "\t\tneg_out = model(n[0].unsqueeze(2).to(device), n[1].unsqueeze(2).to(device))\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss = criterion(pos_out, neg_out)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\ttotal_loss += loss.item() * p[0].size(0)\n",
    "\n",
    "\t\tpbar.set_postfix(loss=f\"{total_loss/N:.2f}\")\n",
    "\n",
    "\tavg_loss = total_loss / N\n",
    "\tlog.append(avg_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = testing_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'reuters',\n",
       " 1: 'short',\n",
       " 2: 'sellers',\n",
       " 3: 'wall',\n",
       " 4: 'street',\n",
       " 5: 's',\n",
       " 6: 'dwindling',\n",
       " 7: 'band',\n",
       " 8: 'of',\n",
       " 9: 'ultra',\n",
       " 10: 'cynics',\n",
       " 11: 'are',\n",
       " 12: 'seeing',\n",
       " 13: 'green',\n",
       " 14: 'again',\n",
       " 15: 'private',\n",
       " 16: 'investment',\n",
       " 17: 'firm',\n",
       " 18: 'carlyle',\n",
       " 19: 'group',\n",
       " 20: 'which',\n",
       " 21: 'has',\n",
       " 22: 'a',\n",
       " 23: 'reputation',\n",
       " 24: 'for',\n",
       " 25: 'making',\n",
       " 26: 'well',\n",
       " 27: 'timed',\n",
       " 28: 'and',\n",
       " 29: 'occasionally',\n",
       " 30: 'controversial',\n",
       " 31: 'plays',\n",
       " 32: 'in',\n",
       " 33: 'the',\n",
       " 34: 'defense',\n",
       " 35: 'industry',\n",
       " 36: 'quietly',\n",
       " 37: 'placed',\n",
       " 38: 'its',\n",
       " 39: 'bets',\n",
       " 40: 'on',\n",
       " 41: 'another',\n",
       " 42: 'part',\n",
       " 43: 'market',\n",
       " 44: 'soaring',\n",
       " 45: 'crude',\n",
       " 46: 'prices',\n",
       " 47: 'plus',\n",
       " 48: 'worries',\n",
       " 49: 'about',\n",
       " 50: 'economy',\n",
       " 51: 'outlook',\n",
       " 52: 'earnings',\n",
       " 53: 'expected',\n",
       " 54: 'to',\n",
       " 55: 'hang',\n",
       " 56: 'over',\n",
       " 57: 'stock',\n",
       " 58: 'next',\n",
       " 59: 'week',\n",
       " 60: 'during',\n",
       " 61: 'depth',\n",
       " 62: 'summer',\n",
       " 63: 'doldrums',\n",
       " 64: 'authorities',\n",
       " 65: 'have',\n",
       " 66: 'halted',\n",
       " 67: 'oil',\n",
       " 68: 'export',\n",
       " 69: 'flows',\n",
       " 70: 'from',\n",
       " 71: 'main',\n",
       " 72: 'pipeline',\n",
       " 73: 'southern',\n",
       " 74: 'iraq',\n",
       " 75: 'after',\n",
       " 76: 'intelligence',\n",
       " 77: 'showed',\n",
       " 78: 'rebel',\n",
       " 79: 'militia',\n",
       " 80: 'could',\n",
       " 81: 'strike',\n",
       " 82: 'infrastructure',\n",
       " 83: 'an',\n",
       " 84: 'official',\n",
       " 85: 'said',\n",
       " 86: 'saturday',\n",
       " 87: 'afp',\n",
       " 88: 'tearaway',\n",
       " 89: 'world',\n",
       " 90: 'toppling',\n",
       " 91: 'records',\n",
       " 92: 'straining',\n",
       " 93: 'wallets',\n",
       " 94: 'present',\n",
       " 95: 'new',\n",
       " 96: 'economic',\n",
       " 97: 'menace',\n",
       " 98: 'barely',\n",
       " 99: 'three',\n",
       " 100: 'months',\n",
       " 101: 'before',\n",
       " 102: 'us',\n",
       " 103: 'presidential',\n",
       " 104: 'elections',\n",
       " 105: 'stocks',\n",
       " 106: 'ended',\n",
       " 107: 'slightly',\n",
       " 108: 'higher',\n",
       " 109: 'friday',\n",
       " 110: 'but',\n",
       " 111: 'stayed',\n",
       " 112: 'near',\n",
       " 113: 'lows',\n",
       " 114: 'year',\n",
       " 115: 'as',\n",
       " 116: 'surged',\n",
       " 117: 'past',\n",
       " 118: '<num>',\n",
       " 119: 'barrel',\n",
       " 120: 'offsetting',\n",
       " 121: 'positive',\n",
       " 122: 'computer',\n",
       " 123: 'maker',\n",
       " 124: 'dell',\n",
       " 125: 'inc',\n",
       " 126: 'o',\n",
       " 127: 'ap',\n",
       " 128: 'assets',\n",
       " 129: 'nation',\n",
       " 130: 'retail',\n",
       " 131: 'money',\n",
       " 132: 'mutual',\n",
       " 133: 'funds',\n",
       " 134: 'fell',\n",
       " 135: 'by',\n",
       " 136: 'billion',\n",
       " 137: 'latest',\n",
       " 138: 'trillion',\n",
       " 139: 'company',\n",
       " 140: 'institute',\n",
       " 141: 'thursday',\n",
       " 142: 'usatoday',\n",
       " 143: 'com',\n",
       " 144: 'sales',\n",
       " 145: 'bounced',\n",
       " 146: 'back',\n",
       " 147: 'bit',\n",
       " 148: 'july',\n",
       " 149: 'claims',\n",
       " 150: 'jobless',\n",
       " 151: 'benefits',\n",
       " 152: 'last',\n",
       " 153: 'government',\n",
       " 154: 'indicating',\n",
       " 155: 'is',\n",
       " 156: 'improving',\n",
       " 157: 'midsummer',\n",
       " 158: 'slump',\n",
       " 159: 'forbes',\n",
       " 160: 'earning',\n",
       " 161: 'ph',\n",
       " 162: 'd',\n",
       " 163: 'sociology',\n",
       " 164: 'danny',\n",
       " 165: 'bazil',\n",
       " 166: 'riley',\n",
       " 167: 'started',\n",
       " 168: 'work',\n",
       " 169: 'general',\n",
       " 170: 'manager',\n",
       " 171: 'at',\n",
       " 172: 'commercial',\n",
       " 173: 'real',\n",
       " 174: 'estate',\n",
       " 175: 'annual',\n",
       " 176: 'base',\n",
       " 177: 'salary',\n",
       " 178: 'soon',\n",
       " 179: 'financial',\n",
       " 180: 'planner',\n",
       " 181: 'stopped',\n",
       " 182: 'his',\n",
       " 183: 'desk',\n",
       " 184: 'drop',\n",
       " 185: 'off',\n",
       " 186: 'brochures',\n",
       " 187: 'insurance',\n",
       " 188: 'available',\n",
       " 189: 'through',\n",
       " 190: 'employer',\n",
       " 191: 'buying',\n",
       " 192: 'was',\n",
       " 193: 'furthest',\n",
       " 194: 'thing',\n",
       " 195: 'my',\n",
       " 196: 'mind',\n",
       " 197: 'says',\n",
       " 198: 'york',\n",
       " 199: 'tehran',\n",
       " 200: 'opec',\n",
       " 201: 'can',\n",
       " 202: 'do',\n",
       " 203: 'nothing',\n",
       " 204: 'douse',\n",
       " 205: 'scorching',\n",
       " 206: 'when',\n",
       " 207: 'markets',\n",
       " 208: 'already',\n",
       " 209: 'oversupplied',\n",
       " 210: 'million',\n",
       " 211: 'barrels',\n",
       " 212: 'per',\n",
       " 213: 'day',\n",
       " 214: 'bpd',\n",
       " 215: 'iran',\n",
       " 216: 'governor',\n",
       " 217: 'warning',\n",
       " 218: 'that',\n",
       " 219: 'fall',\n",
       " 220: 'sharply',\n",
       " 221: 'jakarta',\n",
       " 222: 'non',\n",
       " 223: 'exporters',\n",
       " 224: 'should',\n",
       " 225: 'consider',\n",
       " 226: 'increasing',\n",
       " 227: 'output',\n",
       " 228: 'cool',\n",
       " 229: 'record',\n",
       " 230: 'president',\n",
       " 231: 'purnomo',\n",
       " 232: 'yusgiantoro',\n",
       " 233: 'sunday',\n",
       " 234: 'washington',\n",
       " 235: 'auction',\n",
       " 236: 'google',\n",
       " 237: 'highly',\n",
       " 238: 'anticipated',\n",
       " 239: 'initial',\n",
       " 240: 'public',\n",
       " 241: 'offering',\n",
       " 242: 'got',\n",
       " 243: 'rocky',\n",
       " 244: 'start',\n",
       " 245: 'web',\n",
       " 246: 'search',\n",
       " 247: 'sidestepped',\n",
       " 248: 'bullet',\n",
       " 249: 'u',\n",
       " 250: 'securities',\n",
       " 251: 'regulators',\n",
       " 252: 'dollar',\n",
       " 253: 'tumbled',\n",
       " 254: 'broadly',\n",
       " 255: 'data',\n",
       " 256: 'showing',\n",
       " 257: 'trade',\n",
       " 258: 'deficit',\n",
       " 259: 'june',\n",
       " 260: 'cast',\n",
       " 261: 'fresh',\n",
       " 262: 'doubts',\n",
       " 263: 'recovery',\n",
       " 264: 'ability',\n",
       " 265: 'draw',\n",
       " 266: 'foreign',\n",
       " 267: 'capital',\n",
       " 268: 'fund',\n",
       " 269: 'growing',\n",
       " 270: 'gap',\n",
       " 271: 'if',\n",
       " 272: 'you',\n",
       " 273: 'think',\n",
       " 274: 'may',\n",
       " 275: 'need',\n",
       " 276: 'help',\n",
       " 277: 'your',\n",
       " 278: 'elderly',\n",
       " 279: 'relatives',\n",
       " 280: 'with',\n",
       " 281: 'their',\n",
       " 282: 'finances',\n",
       " 283: 'don',\n",
       " 284: 't',\n",
       " 285: 'be',\n",
       " 286: 'shy',\n",
       " 287: 'having',\n",
       " 288: 'talk',\n",
       " 289: 'purchasing',\n",
       " 290: 'power',\n",
       " 291: 'kids',\n",
       " 292: 'big',\n",
       " 293: 'why',\n",
       " 294: 'school',\n",
       " 295: 'season',\n",
       " 296: 'become',\n",
       " 297: 'such',\n",
       " 298: 'huge',\n",
       " 299: 'marketing',\n",
       " 300: 'phenomenon',\n",
       " 301: 'there',\n",
       " 302: 'little',\n",
       " 303: 'cause',\n",
       " 304: 'celebration',\n",
       " 305: 'these',\n",
       " 306: 'days',\n",
       " 307: 'investors',\n",
       " 308: 'value',\n",
       " 309: 'focused',\n",
       " 310: 'reason',\n",
       " 311: 'feel',\n",
       " 312: 'smug',\n",
       " 313: 'only',\n",
       " 314: 'because',\n",
       " 315: 'they',\n",
       " 316: 've',\n",
       " 317: 'lost',\n",
       " 318: 'less',\n",
       " 319: 'than',\n",
       " 320: 'folks',\n",
       " 321: 'who',\n",
       " 322: 'stuck',\n",
       " 323: 'growth',\n",
       " 324: 'exploded',\n",
       " 325: 'bn',\n",
       " 326: 'costs',\n",
       " 327: 'drove',\n",
       " 328: 'imports',\n",
       " 329: 'according',\n",
       " 330: 'figures',\n",
       " 331: 'giant',\n",
       " 332: 'shell',\n",
       " 333: 'bracing',\n",
       " 334: 'itself',\n",
       " 335: 'takeover',\n",
       " 336: 'attempt',\n",
       " 337: 'possibly',\n",
       " 338: 'french',\n",
       " 339: 'rival',\n",
       " 340: 'total',\n",
       " 341: 'press',\n",
       " 342: 'report',\n",
       " 343: 'bidding',\n",
       " 344: 'gets',\n",
       " 345: 'underway',\n",
       " 346: 'despite',\n",
       " 347: 'minute',\n",
       " 348: 'interview',\n",
       " 349: 'bosses',\n",
       " 350: 'playboy',\n",
       " 351: 'magazine',\n",
       " 352: 'show',\n",
       " 353: 'eurozone',\n",
       " 354: 'continues',\n",
       " 355: 'grow',\n",
       " 356: 'warnings',\n",
       " 357: 'it',\n",
       " 358: 'slow',\n",
       " 359: 'down',\n",
       " 360: 'later',\n",
       " 361: 'japan',\n",
       " 362: 'slows',\n",
       " 363: 'country',\n",
       " 364: 'experiences',\n",
       " 365: 'domestic',\n",
       " 366: 'corporate',\n",
       " 367: 'spending',\n",
       " 368: 'interest',\n",
       " 369: 'rates',\n",
       " 370: 'trimmed',\n",
       " 371: 'south',\n",
       " 372: 'african',\n",
       " 373: 'central',\n",
       " 374: 'bank',\n",
       " 375: 'lack',\n",
       " 376: 'hits',\n",
       " 377: 'rand',\n",
       " 378: 'surprises',\n",
       " 379: 'cost',\n",
       " 380: 'both',\n",
       " 381: 'second',\n",
       " 382: 'hand',\n",
       " 383: 'cars',\n",
       " 384: 'five',\n",
       " 385: 'years',\n",
       " 386: 'survey',\n",
       " 387: 'found',\n",
       " 388: 'korea',\n",
       " 389: 'cuts',\n",
       " 390: 'quarter',\n",
       " 391: 'percentage',\n",
       " 392: 'point',\n",
       " 393: 'bid',\n",
       " 394: 'drive',\n",
       " 395: 'shares',\n",
       " 396: 'engine',\n",
       " 397: 'floated',\n",
       " 398: 'much',\n",
       " 399: '<num>bn',\n",
       " 400: 'takes',\n",
       " 401: 'place',\n",
       " 402: 'hewlett',\n",
       " 403: 'packard',\n",
       " 404: 'disappointing',\n",
       " 405: 'third',\n",
       " 406: 'profits',\n",
       " 407: 'while',\n",
       " 408: 'warns',\n",
       " 409: 'final',\n",
       " 410: 'will',\n",
       " 411: 'also',\n",
       " 412: 'expectations',\n",
       " 413: 'one',\n",
       " 414: 'oldest',\n",
       " 415: 'textile',\n",
       " 416: 'operators',\n",
       " 417: 'indian',\n",
       " 418: 'ocean',\n",
       " 419: 'island',\n",
       " 420: 'mauritius',\n",
       " 421: 'shut',\n",
       " 422: 'seven',\n",
       " 423: 'factories',\n",
       " 424: 'cut',\n",
       " 425: 'jobs',\n",
       " 426: 'chad',\n",
       " 427: 'asks',\n",
       " 428: 'imf',\n",
       " 429: 'loan',\n",
       " 430: 'pay',\n",
       " 431: 'looking',\n",
       " 432: 'more',\n",
       " 433: 'refugees',\n",
       " 434: 'conflict',\n",
       " 435: 'torn',\n",
       " 436: 'darfur',\n",
       " 437: 'western',\n",
       " 438: 'sudan',\n",
       " 439: 'running',\n",
       " 440: 'japanese',\n",
       " 441: 'nuclear',\n",
       " 442: 'plant',\n",
       " 443: 'hit',\n",
       " 444: 'fatal',\n",
       " 445: 'accident',\n",
       " 446: 'close',\n",
       " 447: 'reactors',\n",
       " 448: 'safety',\n",
       " 449: 'checks',\n",
       " 450: 'trevor',\n",
       " 451: 'baylis',\n",
       " 452: 'veteran',\n",
       " 453: 'inventor',\n",
       " 454: 'famous',\n",
       " 455: 'creating',\n",
       " 456: 'freeplay',\n",
       " 457: 'clockwork',\n",
       " 458: 'radio',\n",
       " 459: 'planning',\n",
       " 460: 'float',\n",
       " 461: 'saudi',\n",
       " 462: 'arabia',\n",
       " 463: 'ready',\n",
       " 464: 'push',\n",
       " 465: 'extra',\n",
       " 466: 'into',\n",
       " 467: 'reverse',\n",
       " 468: 'surging',\n",
       " 469: 'led',\n",
       " 470: 'uae',\n",
       " 471: 'etisalat',\n",
       " 472: 'plans',\n",
       " 473: 'spend',\n",
       " 474: '<num>m',\n",
       " 475: 'expansion',\n",
       " 476: 'winning',\n",
       " 477: 'two',\n",
       " 478: 'mobile',\n",
       " 479: 'phone',\n",
       " 480: 'licences',\n",
       " 481: 'network',\n",
       " 482: 'rail',\n",
       " 483: 'flies',\n",
       " 484: 'specialist',\n",
       " 485: 'engineers',\n",
       " 486: 'west',\n",
       " 487: 'coast',\n",
       " 488: 'mainline',\n",
       " 489: 'uk',\n",
       " 490: 'skills',\n",
       " 491: 'shortage',\n",
       " 492: 'bedford',\n",
       " 493: 'scientists',\n",
       " 494: 'nitromed',\n",
       " 495: 'hope',\n",
       " 496: 'experimental',\n",
       " 497: 'drugs',\n",
       " 498: 'cure',\n",
       " 499: 'heart',\n",
       " 500: 'disease',\n",
       " 501: 'someday',\n",
       " 502: 'lately',\n",
       " 503: 'focus',\n",
       " 504: 'been',\n",
       " 505: 'mundane',\n",
       " 506: 'matters',\n",
       " 507: 'i',\n",
       " 508: 'submitted',\n",
       " 509: 'buy',\n",
       " 510: 'style',\n",
       " 511: 'turn',\n",
       " 512: 'out',\n",
       " 513: 'good',\n",
       " 514: 'news',\n",
       " 515: 'or',\n",
       " 516: 'bad',\n",
       " 517: 'massachusetts',\n",
       " 518: 'bargain',\n",
       " 519: 'hunters',\n",
       " 520: 'up',\n",
       " 521: 'droves',\n",
       " 522: 'shopped',\n",
       " 523: 'hard',\n",
       " 524: 'yesterday',\n",
       " 525: 'tax',\n",
       " 526: 'holiday',\n",
       " 527: 'everything',\n",
       " 528: 'treadmills',\n",
       " 529: 'snow',\n",
       " 530: 'blowers',\n",
       " 531: 'candles',\n",
       " 532: 'chandeliers',\n",
       " 533: 'crediting',\n",
       " 534: 'percent',\n",
       " 535: 'break',\n",
       " 536: 'bringing',\n",
       " 537: 'them',\n",
       " 538: 'stores',\n",
       " 539: 'e',\n",
       " 540: 'mail',\n",
       " 541: 'victim',\n",
       " 542: 'own',\n",
       " 543: 'success',\n",
       " 544: 'conclusion',\n",
       " 545: 'ibm',\n",
       " 546: 'corp',\n",
       " 547: 'researchers',\n",
       " 548: 'cambridge',\n",
       " 549: 'spent',\n",
       " 550: 'nearly',\n",
       " 551: 'decade',\n",
       " 552: 'conducting',\n",
       " 553: 'field',\n",
       " 554: 'tests',\n",
       " 555: 'other',\n",
       " 556: 'companies',\n",
       " 557: 'how',\n",
       " 558: 'employees',\n",
       " 559: 'use',\n",
       " 560: 'electronic',\n",
       " 561: 'clear',\n",
       " 562: 'internet',\n",
       " 563: 'killer',\n",
       " 564: 'application',\n",
       " 565: 'even',\n",
       " 566: 'genius',\n",
       " 567: 'mess',\n",
       " 568: 'bill',\n",
       " 569: 'gates',\n",
       " 570: 'brilliant',\n",
       " 571: 'technologist',\n",
       " 572: 'he',\n",
       " 573: 'cofounded',\n",
       " 574: 'microsoft',\n",
       " 575: 'guided',\n",
       " 576: 'greatness',\n",
       " 577: 'size',\n",
       " 578: 'historical',\n",
       " 579: 'consequence',\n",
       " 580: 'blundered',\n",
       " 581: 'terrorized',\n",
       " 582: 'underlings',\n",
       " 583: 'temper',\n",
       " 584: 'parceled',\n",
       " 585: 'praise',\n",
       " 586: 'like',\n",
       " 587: 'scrooge',\n",
       " 588: 'gave',\n",
       " 589: 'charity',\n",
       " 590: 'lash',\n",
       " 591: 'inspired',\n",
       " 592: 'necessary',\n",
       " 593: 'aggressiveness',\n",
       " 594: 'beat',\n",
       " 595: 'competition',\n",
       " 596: 'thought',\n",
       " 597: 'target',\n",
       " 598: 'abusers',\n",
       " 599: 'legal',\n",
       " 600: 'weapons',\n",
       " 601: 'we',\n",
       " 602: 'all',\n",
       " 603: 'share',\n",
       " 604: 'outrage',\n",
       " 605: 'expressed',\n",
       " 606: 'columnist',\n",
       " 607: 'steve',\n",
       " 608: 'bailey',\n",
       " 609: 'sizzler',\n",
       " 610: 'quot',\n",
       " 611: 'aug',\n",
       " 612: 'killings',\n",
       " 613: 'city',\n",
       " 614: 'poor',\n",
       " 615: 'neighborhoods',\n",
       " 616: 'no',\n",
       " 617: 'ignorance',\n",
       " 618: 'argues',\n",
       " 619: 'renewal',\n",
       " 620: 'so',\n",
       " 621: 'called',\n",
       " 622: 'assault',\n",
       " 623: 'weapon',\n",
       " 624: 'ban',\n",
       " 625: 'claiming',\n",
       " 626: 'otherwise',\n",
       " 627: 'uzis',\n",
       " 628: 'ak',\n",
       " 629: '<num>s',\n",
       " 630: 'flooding',\n",
       " 631: 'streets',\n",
       " 632: 'bush',\n",
       " 633: 'saying',\n",
       " 634: 'turned',\n",
       " 635: 'corner',\n",
       " 636: 'democratic',\n",
       " 637: 'candidate',\n",
       " 638: 'senator',\n",
       " 639: 'john',\n",
       " 640: 'f',\n",
       " 641: 'kerry',\n",
       " 642: 'wake',\n",
       " 643: 'this',\n",
       " 644: 'month',\n",
       " 645: 'quipped',\n",
       " 646: 'marlborough',\n",
       " 647: 'based',\n",
       " 648: 'technology',\n",
       " 649: 'suing',\n",
       " 650: 'former',\n",
       " 651: 'including',\n",
       " 652: 'senior',\n",
       " 653: 'managers',\n",
       " 654: 'allegedly',\n",
       " 655: 'conspiring',\n",
       " 656: 'against',\n",
       " 657: 'working',\n",
       " 658: 'opening',\n",
       " 659: 'competing',\n",
       " 660: 'business',\n",
       " 661: 'square',\n",
       " 662: 'lynn',\n",
       " 663: 'brighter',\n",
       " 664: 'sidewalks',\n",
       " 665: 'curbs',\n",
       " 666: 'fences',\n",
       " 667: 'lights',\n",
       " 668: 'landscaping',\n",
       " 669: 'road',\n",
       " 670: 'improvements',\n",
       " 671: 'planned',\n",
       " 672: 'gateway',\n",
       " 673: 'artisan',\n",
       " 674: 'block',\n",
       " 675: 'key',\n",
       " 676: 'area',\n",
       " 677: 'state',\n",
       " 678: 'grant',\n",
       " 679: 'given',\n",
       " 680: 'lawsuit',\n",
       " 681: 'gary',\n",
       " 682: 'winnick',\n",
       " 683: 'chief',\n",
       " 684: 'global',\n",
       " 685: 'crossing',\n",
       " 686: 'refocuses',\n",
       " 687: 'attention',\n",
       " 688: 'what',\n",
       " 689: 'mr',\n",
       " 690: 'knew',\n",
       " 691: 'imploded',\n",
       " 692: 'russia',\n",
       " 693: 'emerging',\n",
       " 694: 'superpower',\n",
       " 695: 'kevin',\n",
       " 696: 'b',\n",
       " 697: 'rollins',\n",
       " 698: 'executive',\n",
       " 699: 'talks',\n",
       " 700: 'transitory',\n",
       " 701: 'slip',\n",
       " 702: 'customer',\n",
       " 703: 'service',\n",
       " 704: 'sees',\n",
       " 705: 'broader',\n",
       " 706: 'taking',\n",
       " 707: 'cash',\n",
       " 708: 'rich',\n",
       " 709: 'people',\n",
       " 710: 'dying',\n",
       " 711: 'wealth',\n",
       " 712: 'stein',\n",
       " 713: 'proposes',\n",
       " 714: 'unique',\n",
       " 715: 'solution',\n",
       " 716: 'sell',\n",
       " 717: 'titles',\n",
       " 718: 'nobility',\n",
       " 719: 'quality',\n",
       " 720: 'distribution',\n",
       " 721: 'hammered',\n",
       " 722: 'reporting',\n",
       " 723: 'large',\n",
       " 724: 'loss',\n",
       " 725: 'hurricane',\n",
       " 726: 'charley',\n",
       " 727: 'blows',\n",
       " 728: 'house',\n",
       " 729: 'make',\n",
       " 730: 'results',\n",
       " 731: 'not',\n",
       " 732: 'grim',\n",
       " 733: 'tech',\n",
       " 734: 'just',\n",
       " 735: 'isn',\n",
       " 736: 'tough',\n",
       " 737: 'detroit',\n",
       " 738: 'troubled',\n",
       " 739: 'carmaker',\n",
       " 740: 'thanks',\n",
       " 741: 'maverick',\n",
       " 742: 'designer',\n",
       " 743: 'car',\n",
       " 744: 'dazzling',\n",
       " 745: 'hip',\n",
       " 746: 'hop',\n",
       " 747: 'crowd',\n",
       " 748: 'americans',\n",
       " 749: 'tricking',\n",
       " 750: 'places',\n",
       " 751: 'where',\n",
       " 752: 'swim',\n",
       " 753: 'here',\n",
       " 754: 'look',\n",
       " 755: 'wave',\n",
       " 756: 'accessories',\n",
       " 757: 'six',\n",
       " 758: 'geeks',\n",
       " 759: 'had',\n",
       " 760: 'digital',\n",
       " 761: 'nightmare',\n",
       " 762: 'changed',\n",
       " 763: 'culture',\n",
       " 764: 'get',\n",
       " 765: 'far',\n",
       " 766: 'creepier',\n",
       " 767: 'celebrity',\n",
       " 768: 'fashion',\n",
       " 769: 'booming',\n",
       " 770: 'webpreneurs',\n",
       " 771: 'recording',\n",
       " 772: 'artist',\n",
       " 773: 'channel',\n",
       " 774: 'american',\n",
       " 775: 'middle',\n",
       " 776: 'class',\n",
       " 777: 'tastes',\n",
       " 778: 'quite',\n",
       " 779: 'chip',\n",
       " 780: 'davis',\n",
       " 781: 'best',\n",
       " 782: 'selling',\n",
       " 783: 'problem',\n",
       " 784: 'worry',\n",
       " 785: 'find',\n",
       " 786: 'every',\n",
       " 787: 'specialized',\n",
       " 788: 'today',\n",
       " 789: 'customers',\n",
       " 790: 'increasingly',\n",
       " 791: 'demanding',\n",
       " 792: 'asia',\n",
       " 793: 'elsewhere',\n",
       " 794: 'henry',\n",
       " 795: 'astorga',\n",
       " 796: 'describes',\n",
       " 797: 'complex',\n",
       " 798: 'reality',\n",
       " 799: 'faced',\n",
       " 800: 'marketers',\n",
       " 801: 'includes',\n",
       " 802: 'used',\n",
       " 803: 'want',\n",
       " 804: 'performance',\n",
       " 805: 'now',\n",
       " 806: 'election',\n",
       " 807: 'time',\n",
       " 808: 'republic',\n",
       " 809: 'philippines',\n",
       " 810: 'means',\n",
       " 811: 'monkeys',\n",
       " 812: 'rolling',\n",
       " 813: 'around',\n",
       " 814: 'those',\n",
       " 815: 'political',\n",
       " 816: 'fun',\n",
       " 817: 'laughing',\n",
       " 818: 'heads',\n",
       " 819: 'strange',\n",
       " 820: 'goings',\n",
       " 821: 'characterize',\n",
       " 822: 'process',\n",
       " 823: 'loosely',\n",
       " 824: 'model',\n",
       " 825: 'de',\n",
       " 826: 'facto',\n",
       " 827: 'looks',\n",
       " 828: 'fellini',\n",
       " 829: 'movie',\n",
       " 830: 'crossed',\n",
       " 831: 'tom',\n",
       " 832: 'jerry',\n",
       " 833: 'cartoon',\n",
       " 834: 'column',\n",
       " 835: 'useful',\n",
       " 836: 'glossary',\n",
       " 837: 'motors',\n",
       " 838: 'dropped',\n",
       " 839: 'oldsmobile',\n",
       " 840: 'four',\n",
       " 841: 'brand',\n",
       " 842: 'paradoxes',\n",
       " 843: 'gm',\n",
       " 844: 'face',\n",
       " 845: 'name',\n",
       " 846: 'product',\n",
       " 847: 'image',\n",
       " 848: 're',\n",
       " 849: 'positioning',\n",
       " 850: 'consumer',\n",
       " 851: 'added',\n",
       " 852: 'rebranding',\n",
       " 853: 'although',\n",
       " 854: 'smattering',\n",
       " 855: 'chinese',\n",
       " 856: 'filipinos',\n",
       " 857: 'indians',\n",
       " 858: 'thais',\n",
       " 859: 'others',\n",
       " 860: 'crow',\n",
       " 861: 'kind',\n",
       " 862: 'sitting',\n",
       " 863: 'prominent',\n",
       " 864: 'positions',\n",
       " 865: 'corporations',\n",
       " 866: 'organizations',\n",
       " 867: 'usa',\n",
       " 868: 'accomplishments',\n",
       " 869: 'mere',\n",
       " 870: 'cultural',\n",
       " 871: 'high',\n",
       " 872: 'fives',\n",
       " 873: 'ritualistic',\n",
       " 874: 'chest',\n",
       " 875: 'thumping',\n",
       " 876: 'goaded',\n",
       " 877: 'impishly',\n",
       " 878: 'patronized',\n",
       " 879: 'mainstream',\n",
       " 880: 'society',\n",
       " 881: 'milder',\n",
       " 882: 'gentler',\n",
       " 883: 'term',\n",
       " 884: 'white',\n",
       " 885: 'dominated',\n",
       " 886: 'populace',\n",
       " 887: 'blues',\n",
       " 888: 'alive',\n",
       " 889: 'evidenced',\n",
       " 890: 'appreciation',\n",
       " 891: 'pinoy',\n",
       " 892: 'lampano',\n",
       " 893: 'alley',\n",
       " 894: 'penned',\n",
       " 895: 'clarence',\n",
       " 896: 'henderson',\n",
       " 897: 'counterpoint',\n",
       " 898: 'usual',\n",
       " 899: 'economics',\n",
       " 900: 'fare',\n",
       " 901: 'globalization',\n",
       " 902: 'does',\n",
       " 903: 'things',\n",
       " 904: 'life',\n",
       " 905: 'manila',\n",
       " 906: 'consultant',\n",
       " 907: 'proving',\n",
       " 908: 'really',\n",
       " 909: 'muddy',\n",
       " 910: 'walters',\n",
       " 911: 'bluegrass',\n",
       " 912: 'same',\n",
       " 913: 'page',\n",
       " 914: 'apmf',\n",
       " 915: 'asian',\n",
       " 916: 'tourism',\n",
       " 917: 'destinations',\n",
       " 918: 'kicked',\n",
       " 919: 'crowded',\n",
       " 920: 'top',\n",
       " 921: 'chiang',\n",
       " 922: 'mai',\n",
       " 923: 'thailand',\n",
       " 924: 'leading',\n",
       " 925: 'perennial',\n",
       " 926: 'favourites',\n",
       " 927: 'hong',\n",
       " 928: 'kong',\n",
       " 929: 'bangkok',\n",
       " 930: 'phuket',\n",
       " 931: 'bali',\n",
       " 932: 'indonesia',\n",
       " 933: 'first',\n",
       " 934: 'vote',\n",
       " 935: 'let',\n",
       " 936: 'know',\n",
       " 937: 'reasons',\n",
       " 938: 'our',\n",
       " 939: 'categories',\n",
       " 940: 'sense',\n",
       " 941: 'couple',\n",
       " 942: 'singapore',\n",
       " 943: 'leads',\n",
       " 944: 'pack',\n",
       " 945: 'followed',\n",
       " 946: 'enter',\n",
       " 947: 'comments',\n",
       " 948: 'views',\n",
       " 949: 'count',\n",
       " 950: 'include',\n",
       " 951: 'livability',\n",
       " 952: 'grew',\n",
       " 953: 'alligators',\n",
       " 954: 'sometimes',\n",
       " 955: 'ornately',\n",
       " 956: 'described',\n",
       " 957: 'albino',\n",
       " 958: 'were',\n",
       " 959: 'rumored',\n",
       " 960: 'roam',\n",
       " 961: 'citys',\n",
       " 962: 'sewer',\n",
       " 963: 'systems',\n",
       " 964: 'legend',\n",
       " 965: 'vacationers',\n",
       " 966: 'picked',\n",
       " 967: 'tiny',\n",
       " 968: 'crocodilians',\n",
       " 969: 'florida',\n",
       " 970: 'brought',\n",
       " 971: 'home',\n",
       " 972: 'eventually',\n",
       " 973: 'flushed',\n",
       " 974: 'buggers',\n",
       " 975: 'too',\n",
       " 976: 'local',\n",
       " 977: 'concrete',\n",
       " 978: 'jungle',\n",
       " 979: 'most',\n",
       " 980: 'projects',\n",
       " 981: 'fail',\n",
       " 982: 'some',\n",
       " 983: 'number',\n",
       " 984: 'consultancies',\n",
       " 985: 'services',\n",
       " 986: 'capgemini',\n",
       " 987: 'sapient',\n",
       " 988: 'feed',\n",
       " 989: 'encountered',\n",
       " 990: 'enterprises',\n",
       " 991: 'founded',\n",
       " 992: 'realization',\n",
       " 993: 'successful',\n",
       " 994: 'cto',\n",
       " 995: 'ben',\n",
       " 996: 'gaucherin',\n",
       " 997: 'bea',\n",
       " 998: 'hired',\n",
       " 999: 'associates',\n",
       " ...}"
      ]
     },
     "execution_count": 663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_embeddings(model, corpus):\n",
    "# \tnow = datetime.now()\n",
    "# \tlog_dir = f'runs/negativesampling_{now.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "# \twriter = SummaryWriter(log_dir)\n",
    "# \tembeddings = model.center.weight\n",
    "# \tlabels = list(corpus.id2word.values())\n",
    "# \twriter.add_embedding(embeddings, metadata=labels)\n",
    "# \twriter.flush()\n",
    "# \twriter.close()\n",
    "# \treturn log_dir\n",
    "# log_embeddings(model, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runs/negativesampling_20240705-171458'"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log_embeddings(model, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
