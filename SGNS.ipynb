{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = \"data/\"\n",
    "TRAIN = f\"{DATA}/train.csv\"\n",
    "TEST = f\"{DATA}/test.csv\"\n",
    "TRAIN_LABEL = f\"{DATA}/train_label.txt\"\n",
    "TRAIN_TITLE = f\"{DATA}/train_title.txt\"\n",
    "TRAIN_TEXT = f\"{DATA}/train_text.txt\"\n",
    "TEST_LABEL = f\"{DATA}/test_label.txt\"\n",
    "TEST_TITLE = f\"{DATA}/test_title.txt\"\n",
    "TEST_TEXT = f\"{DATA}/test_text.txt\"\n",
    "\n",
    "TRAIN_NORM = f\"{DATA}/train_norm.txt\"\n",
    "TEST_NORM = f\"{DATA}/test_norm.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Splt the csv into label, title, text\n",
    "# import pandas as pd\n",
    "\n",
    "# train = pd.read_csv(TRAIN)\n",
    "# train_col = train.columns\n",
    "# train[train_col[0]].to_csv(TRAIN_LABEL, index=False)\n",
    "# train[train_col[1]].to_csv(TRAIN_TITLE, index=False)\n",
    "# train[train_col[2]].to_csv(TRAIN_TEXT, index=False)\n",
    "\n",
    "# test = pd.read_csv(TEST)\n",
    "# test_col = test.columns\n",
    "# test[test_col[0]].to_csv(TEST_LABEL, index=False)\n",
    "# test[test_col[1]].to_csv(TEST_TITLE, index=False)\n",
    "# test[test_col[2]].to_csv(TEST_TEXT, index=False)\n",
    "# del train, test, pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.Normalizer import normalize_data\n",
    "# normalize_data(TRAIN_TEXT, TRAIN_NORM)\n",
    "# normalize_data(TEST_TEXT, TEST_NORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Union, Generator, Dict\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "from collections import Counter\n",
    "from datetime import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "\tdef __init__(self, file_name: str, window: int = 1, k = 3):\n",
    "\t\tself.file_name = file_name\n",
    "\t\tself.word2id: Dict[str, int] = {}\n",
    "\t\tself.id2word: Dict[int, str] = {}\n",
    "\t\tself.word_count = {}\n",
    "\t\tself.k = k\n",
    "\t\tself.vocab_size = 0\n",
    "\t\tself.window = window\n",
    "\t\tself.pairs = []\n",
    "\t\tself.noise_dist = np.array([])\n",
    "\t\tself.__init_data()\n",
    "\t\tself.__subsample()\n",
    "\t\tself.__make_pairs()\n",
    "\t\tself.__noise_dist()\n",
    "\n",
    "\tdef __init_data(self):\n",
    "\t\twith open(self.file_name, \"r\") as file:\n",
    "\t\t\tdata = file.read()\n",
    "\t\tdata = data.split()\n",
    "\t\tself.word_count = Counter(data)\n",
    "\t\tself.total = sum(list(self.word_count.values()))\n",
    "\t\tself.vocab_size = len(word_count)\n",
    "\n",
    "\tdef __subsample(self):\n",
    "\t\tt = 1e-5\n",
    "\t\tsorted_count = sorted(self.word_count, key=self.word_count.get, reverse=True)  # type: ignore\n",
    "\t\tfreq = {word: count / self.total for word, count in self.word_count.items()}\n",
    "\t\tpdrop = {word: 1 - np.sqrt(t / freq[word]) for word in sorted_count}\n",
    "\t\ti = 0\n",
    "\t\t_new_word_count = {}\n",
    "\t\tfor word in sorted_count:\n",
    "\t\t\tif np.random.random() < 1 - pdrop[word]:\n",
    "\t\t\t\tself.id2word[i] = word\n",
    "\t\t\t\tself.word2id[word] = i\n",
    "\t\t\t\t_new_word_count[word] = self.word_count[word]\n",
    "\t\t\t\ti += 1\n",
    "\t\tself.word_count = _new_word_count\n",
    "\t\tself.vocab_size =  len(self.word_count.keys())\n",
    "\t\tself.total = sum(list(self.word_count.values()))\n",
    "\n",
    "\tdef __get_pairs(self, text: List[str]):\n",
    "\t\tnum_words = len(text)\n",
    "\t\tfor i, word in enumerate(text):\n",
    "\t\t\tif word in self.word2id.keys():\n",
    "\t\t\t\tcenter = self.word2id[word]\n",
    "\t\t\t\tcontext = text[max(0, i - self.window) : i]\n",
    "\t\t\t\tcontext += text[i + 1 : min(num_words, i + self.window + 1)]\n",
    "\t\t\t\t_tmp = ((center, self.word2id[cnt]) for cnt in context if cnt in self.word2id.keys())\n",
    "\t\t\t\tself.pairs.extend(_tmp)  # type: ignore\n",
    "\n",
    "\tdef __noise_dist(self,):\n",
    "\t\tfreq = {}\n",
    "\t\tfor word in self.word2id.keys():\n",
    "\t\t\tfreq[word] = self.word_count[word]/self.total\n",
    "\t\tunigram = np.array(list(freq.values()))**(3/4)\n",
    "\t\tself.noise_dist = unigram/unigram.sum()\n",
    "\t\tself.noise_dist = torch.from_numpy(self.noise_dist)\n",
    "\t\t_neg = []\n",
    "\t\tfor pair in self.pairs:\n",
    "\t\t\t_, b = pair\n",
    "\t\t\t_tmp = self.noise_dist.clone()\n",
    "\t\t\t_tmp[b] = 0.0\n",
    "\t\t\t_neg.append(torch.multinomial(input = _tmp, num_samples = self.k, replacement = True))\n",
    "\t\tself.neg = np.asarray(_neg)\n",
    "\t\ti,j = self.pairs.shape\n",
    "\t\tself.pairs = self.pairs.reshape(i,j,1)\n",
    "\n",
    "\t\t\n",
    "\tdef __make_pairs(self):\n",
    "\t\twith open(self.file_name) as data:\n",
    "\t\t\tfor text in data:\n",
    "\t\t\t\tself.__get_pairs(text.strip().split())\n",
    "\t\t_tmp = set(self.pairs)\n",
    "\t\t_tmp = np.array(list(_tmp))\n",
    "\t\tself.pairs = np.array(_tmp)\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNS(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim) -> None:\n",
    "        super(SGNS, self).__init__()\n",
    "        self.vocab_size = vocab_size  # N\n",
    "        self.emb_dim = emb_dim  # H\n",
    "        self.vEmbedding = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "        self.uEmbedding = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "\n",
    "    def forward(self, c, o, neg):\n",
    "        vv = self.vEmbedding(c)  # BxH\n",
    "        uu = self.uEmbedding(o)  # BxH\n",
    "        ng = self.uEmbedding(neg) # BxKxH\n",
    "        pos = torch.sigmoid(torch.einsum(\"bki,bik->bk\", uu, vv.mT)) # Bx1\n",
    "        neg = torch.sigmoid(torch.einsum(\"bjkl,blk->bjk\", ng, vv.mT).neg()) #BxKx1\n",
    "        return pos, neg\n",
    "\n",
    "\n",
    "class SGNSLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SGNSLoss, self).__init__()\n",
    "\n",
    "    def forward(self, positive, negatives):\n",
    "        a = torch.log(positive).neg()\n",
    "        b = torch.log(negatives).neg().sum(1)\n",
    "        return torch.mean(a + b)\n",
    "\n",
    "\n",
    "class CorpusData(Dataset):\n",
    "    def __init__(self, corpus):\n",
    "        self.pairs = torch.from_numpy(corpus.pairs)\n",
    "        self.negative = torch.from_numpy(corpus.neg)\n",
    "        self.V = corpus.vocab_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        c_positive, o_positive = self.pairs[idx]\n",
    "        o_negative = self.negative[idx]\n",
    "        return c_positive, o_positive, o_negative.unsqueeze(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus(TRAIN_NORM, 3, 10)\n",
    "corpusdata = CorpusData(corpus)\n",
    "V = corpus.vocab_size\n",
    "H = 50\n",
    "device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SGNS(V,H)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SGNSLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "dataloader = DataLoader(corpusdata, batch_size=2*8192, shuffle=True)\n",
    "N = len(corpusdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 52/52 [00:06<00:00,  7.49it/s, loss=1.81]\n",
      "Epoch 2: 100%|██████████| 52/52 [00:06<00:00,  7.84it/s, loss=1.80]\n",
      "Epoch 3: 100%|██████████| 52/52 [00:06<00:00,  8.16it/s, loss=1.79]\n",
      "Epoch 4: 100%|██████████| 52/52 [00:06<00:00,  7.81it/s, loss=1.78]\n",
      "Epoch 5: 100%|██████████| 52/52 [00:06<00:00,  8.12it/s, loss=1.77]\n",
      "Epoch 6: 100%|██████████| 52/52 [00:06<00:00,  8.10it/s, loss=1.77]\n",
      "Epoch 7: 100%|██████████| 52/52 [00:06<00:00,  8.06it/s, loss=1.76]\n",
      "Epoch 8: 100%|██████████| 52/52 [00:06<00:00,  8.05it/s, loss=1.75]\n",
      "Epoch 9: 100%|██████████| 52/52 [00:06<00:00,  8.04it/s, loss=1.74]\n",
      "Epoch 10: 100%|██████████| 52/52 [00:06<00:00,  7.92it/s, loss=1.73]\n",
      "Epoch 11: 100%|██████████| 52/52 [00:06<00:00,  8.09it/s, loss=1.73]\n",
      "Epoch 12: 100%|██████████| 52/52 [00:06<00:00,  8.06it/s, loss=1.72]\n",
      "Epoch 13: 100%|██████████| 52/52 [00:06<00:00,  7.99it/s, loss=1.71]\n",
      "Epoch 14: 100%|██████████| 52/52 [00:06<00:00,  8.06it/s, loss=1.70]\n",
      "Epoch 15: 100%|██████████| 52/52 [00:06<00:00,  7.99it/s, loss=1.69]\n",
      "Epoch 16: 100%|██████████| 52/52 [00:06<00:00,  7.95it/s, loss=1.69]\n",
      "Epoch 17: 100%|██████████| 52/52 [00:06<00:00,  8.01it/s, loss=1.68]\n",
      "Epoch 18: 100%|██████████| 52/52 [00:06<00:00,  7.96it/s, loss=1.67]\n",
      "Epoch 19: 100%|██████████| 52/52 [00:06<00:00,  7.89it/s, loss=1.67]\n",
      "Epoch 20: 100%|██████████| 52/52 [00:06<00:00,  7.89it/s, loss=1.66]\n",
      "Epoch 21: 100%|██████████| 52/52 [00:06<00:00,  7.89it/s, loss=1.65]\n",
      "Epoch 22: 100%|██████████| 52/52 [00:06<00:00,  7.88it/s, loss=1.65]\n",
      "Epoch 23: 100%|██████████| 52/52 [00:06<00:00,  7.89it/s, loss=1.64]\n",
      "Epoch 24: 100%|██████████| 52/52 [00:06<00:00,  7.93it/s, loss=1.63]\n",
      "Epoch 25: 100%|██████████| 52/52 [00:06<00:00,  7.95it/s, loss=1.63]\n",
      "Epoch 26: 100%|██████████| 52/52 [00:06<00:00,  7.87it/s, loss=1.62]\n",
      "Epoch 27: 100%|██████████| 52/52 [00:06<00:00,  7.95it/s, loss=1.61]\n",
      "Epoch 28: 100%|██████████| 52/52 [00:06<00:00,  7.90it/s, loss=1.61]\n",
      "Epoch 29: 100%|██████████| 52/52 [00:06<00:00,  7.87it/s, loss=1.60]\n",
      "Epoch 30: 100%|██████████| 52/52 [00:06<00:00,  7.94it/s, loss=1.59]\n",
      "Epoch 31: 100%|██████████| 52/52 [00:06<00:00,  7.89it/s, loss=1.59]\n",
      "Epoch 32: 100%|██████████| 52/52 [00:06<00:00,  7.90it/s, loss=1.58]\n",
      "Epoch 33: 100%|██████████| 52/52 [00:06<00:00,  7.84it/s, loss=1.58]\n",
      "Epoch 34: 100%|██████████| 52/52 [00:06<00:00,  7.85it/s, loss=1.57]\n",
      "Epoch 35: 100%|██████████| 52/52 [00:06<00:00,  7.93it/s, loss=1.56]\n",
      "Epoch 36: 100%|██████████| 52/52 [00:06<00:00,  7.71it/s, loss=1.56]\n",
      "Epoch 37: 100%|██████████| 52/52 [00:07<00:00,  7.34it/s, loss=1.55]\n",
      "Epoch 38: 100%|██████████| 52/52 [00:07<00:00,  6.91it/s, loss=1.55]\n",
      "Epoch 39: 100%|██████████| 52/52 [00:07<00:00,  7.36it/s, loss=1.54]\n",
      "Epoch 40: 100%|██████████| 52/52 [00:07<00:00,  7.42it/s, loss=1.54]\n",
      "Epoch 41: 100%|██████████| 52/52 [00:06<00:00,  7.50it/s, loss=1.53]\n",
      "Epoch 42: 100%|██████████| 52/52 [00:06<00:00,  7.58it/s, loss=1.52]\n",
      "Epoch 43: 100%|██████████| 52/52 [00:06<00:00,  7.50it/s, loss=1.52]\n",
      "Epoch 44: 100%|██████████| 52/52 [00:07<00:00,  7.15it/s, loss=1.51]\n",
      "Epoch 45: 100%|██████████| 52/52 [00:06<00:00,  7.59it/s, loss=1.51]\n",
      "Epoch 46: 100%|██████████| 52/52 [00:07<00:00,  7.03it/s, loss=1.50]\n",
      "Epoch 47: 100%|██████████| 52/52 [00:06<00:00,  7.47it/s, loss=1.50]\n",
      "Epoch 48: 100%|██████████| 52/52 [00:06<00:00,  7.60it/s, loss=1.49]\n",
      "Epoch 49: 100%|██████████| 52/52 [00:06<00:00,  7.61it/s, loss=1.49]\n",
      "Epoch 50: 100%|██████████| 52/52 [00:06<00:00,  7.57it/s, loss=1.48]\n",
      "Epoch 51: 100%|██████████| 52/52 [00:07<00:00,  7.20it/s, loss=1.48]\n",
      "Epoch 52: 100%|██████████| 52/52 [00:06<00:00,  7.54it/s, loss=1.47]\n",
      "Epoch 53: 100%|██████████| 52/52 [00:07<00:00,  7.39it/s, loss=1.47]\n",
      "Epoch 54: 100%|██████████| 52/52 [00:07<00:00,  7.27it/s, loss=1.46]\n",
      "Epoch 55: 100%|██████████| 52/52 [00:06<00:00,  7.44it/s, loss=1.46]\n",
      "Epoch 56: 100%|██████████| 52/52 [00:07<00:00,  7.06it/s, loss=1.46]\n",
      "Epoch 57: 100%|██████████| 52/52 [00:06<00:00,  7.52it/s, loss=1.45]\n",
      "Epoch 58: 100%|██████████| 52/52 [00:07<00:00,  7.37it/s, loss=1.45]\n",
      "Epoch 59: 100%|██████████| 52/52 [00:07<00:00,  7.41it/s, loss=1.44]\n",
      "Epoch 60: 100%|██████████| 52/52 [00:07<00:00,  7.24it/s, loss=1.44]\n",
      "Epoch 61: 100%|██████████| 52/52 [00:06<00:00,  7.48it/s, loss=1.43]\n",
      "Epoch 62: 100%|██████████| 52/52 [00:07<00:00,  7.15it/s, loss=1.43]\n",
      "Epoch 63: 100%|██████████| 52/52 [00:07<00:00,  7.35it/s, loss=1.42]\n",
      "Epoch 64: 100%|██████████| 52/52 [00:06<00:00,  7.66it/s, loss=1.42]\n",
      "Epoch 65: 100%|██████████| 52/52 [00:07<00:00,  7.37it/s, loss=1.42]\n",
      "Epoch 66: 100%|██████████| 52/52 [00:06<00:00,  7.65it/s, loss=1.41]\n",
      "Epoch 67: 100%|██████████| 52/52 [00:06<00:00,  7.53it/s, loss=1.41]\n",
      "Epoch 68: 100%|██████████| 52/52 [00:06<00:00,  7.57it/s, loss=1.40]\n",
      "Epoch 69: 100%|██████████| 52/52 [00:06<00:00,  7.54it/s, loss=1.40]\n",
      "Epoch 70: 100%|██████████| 52/52 [00:06<00:00,  7.67it/s, loss=1.40]\n",
      "Epoch 71: 100%|██████████| 52/52 [00:06<00:00,  7.68it/s, loss=1.39]\n",
      "Epoch 72: 100%|██████████| 52/52 [00:06<00:00,  7.81it/s, loss=1.39]\n",
      "Epoch 73: 100%|██████████| 52/52 [00:06<00:00,  7.55it/s, loss=1.38]\n",
      "Epoch 74: 100%|██████████| 52/52 [00:06<00:00,  7.49it/s, loss=1.38]\n",
      "Epoch 75: 100%|██████████| 52/52 [00:06<00:00,  7.53it/s, loss=1.38]\n",
      "Epoch 76: 100%|██████████| 52/52 [00:06<00:00,  7.85it/s, loss=1.37]\n",
      "Epoch 77: 100%|██████████| 52/52 [00:06<00:00,  7.80it/s, loss=1.37]\n",
      "Epoch 78: 100%|██████████| 52/52 [00:06<00:00,  7.80it/s, loss=1.37]\n",
      "Epoch 79: 100%|██████████| 52/52 [00:06<00:00,  7.75it/s, loss=1.36]\n",
      "Epoch 80: 100%|██████████| 52/52 [00:06<00:00,  8.09it/s, loss=1.36]\n",
      "Epoch 81: 100%|██████████| 52/52 [00:06<00:00,  8.18it/s, loss=1.35]\n",
      "Epoch 82: 100%|██████████| 52/52 [00:06<00:00,  8.26it/s, loss=1.35]\n",
      "Epoch 83: 100%|██████████| 52/52 [00:06<00:00,  8.23it/s, loss=1.35]\n",
      "Epoch 84: 100%|██████████| 52/52 [00:06<00:00,  8.08it/s, loss=1.34]\n",
      "Epoch 85: 100%|██████████| 52/52 [00:06<00:00,  8.02it/s, loss=1.34]\n",
      "Epoch 86: 100%|██████████| 52/52 [00:06<00:00,  8.00it/s, loss=1.34]\n",
      "Epoch 87: 100%|██████████| 52/52 [00:06<00:00,  8.09it/s, loss=1.33]\n",
      "Epoch 88: 100%|██████████| 52/52 [00:06<00:00,  8.10it/s, loss=1.33]\n",
      "Epoch 89: 100%|██████████| 52/52 [00:06<00:00,  8.08it/s, loss=1.33]\n",
      "Epoch 90: 100%|██████████| 52/52 [00:06<00:00,  8.26it/s, loss=1.32]\n",
      "Epoch 91: 100%|██████████| 52/52 [00:06<00:00,  8.27it/s, loss=1.32]\n",
      "Epoch 92: 100%|██████████| 52/52 [00:06<00:00,  8.36it/s, loss=1.32]\n",
      "Epoch 93: 100%|██████████| 52/52 [00:06<00:00,  8.32it/s, loss=1.31]\n",
      "Epoch 94: 100%|██████████| 52/52 [00:06<00:00,  8.30it/s, loss=1.31]\n",
      "Epoch 95: 100%|██████████| 52/52 [00:06<00:00,  8.24it/s, loss=1.31]\n",
      "Epoch 96: 100%|██████████| 52/52 [00:06<00:00,  8.28it/s, loss=1.30]\n",
      "Epoch 97: 100%|██████████| 52/52 [00:06<00:00,  8.30it/s, loss=1.30]\n",
      "Epoch 98: 100%|██████████| 52/52 [00:06<00:00,  8.19it/s, loss=1.30]\n",
      "Epoch 99: 100%|██████████| 52/52 [00:06<00:00,  7.88it/s, loss=1.30]\n",
      "Epoch 100: 100%|██████████| 52/52 [00:06<00:00,  8.11it/s, loss=1.29]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "\tlog = []\n",
    "\ttotal_loss = 0\n",
    "\tmodel.train()\n",
    "\tpbar = tqdm(dataloader, total=len(dataloader), desc=f\"Epoch {i+1}\")\n",
    "\tfor c, o, n in pbar:\n",
    "\t\tc, o, n = c.to(device), o.to(device), n.to(device)\n",
    "\t\tpos, neg = model(c,o,n)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tloss = criterion(pos, neg)\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\t\ttotal_loss += loss.item() * c.size(0)\n",
    "\t\t# break\n",
    "\t\tpbar.set_postfix(loss=f\"{total_loss/N:.2f}\")\n",
    "\n",
    "\tavg_loss = total_loss / N\n",
    "\tlog.append(avg_loss)\n",
    "\t# break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([55085, 50])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.vEmbedding.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2024, 7, 6, 9, 38, 7, 609074)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'runs/negativesampling_20240706-093845'"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log_embeddings(model, corpus):\n",
    "\tnow = datetime.now()\n",
    "\tlog_dir = f'runs/negativesampling_{now.strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "\twriter = SummaryWriter(log_dir)\n",
    "\tembeddings = model.vEmbedding.weight\n",
    "\tlabels = list(corpus.id2word.values())\n",
    "\twriter.add_embedding(embeddings, metadata=labels)\n",
    "\twriter.flush()\n",
    "\twriter.close()\n",
    "\treturn log_dir\n",
    "log_embeddings(model, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
